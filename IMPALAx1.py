# -*- coding: utf-8 -*-
"""impalax2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g3KYE4aOpBI0JoNmOflhSk6qlsZLVARu

# Getting started with PPO and ProcGen

Here's a bit of code that should help you get started on your projects.

The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details.
"""

#!pip install procgen
#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py

import utils
import os
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from utils import make_env, Storage, orthogonal_init
import imageio
# from google.colab import files

"""Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."""

# Hyperparameters
total_steps = 8e6
num_envs = 32
num_levels = 100
num_steps = 256
num_epochs = 3
batch_size = 512
eps = .2
grad_eps = .5
value_coef = .5
entropy_coef = .01

"""Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."""


class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)


class Encoder(nn.Module):
  def __init__(self, in_channels, feature_dim):
    super().__init__()
    self.feat_convs = []
    self.resnet1 = []
    self.resnet2 = []

    self.convs = []
    input_channels = in_channels 
    for num_ch in [16, 32, 32]:
        feats_convs = []
        feats_convs.append(
            nn.Conv2d(
                in_channels=input_channels,
                out_channels=num_ch,
                kernel_size=3,
                stride=1,
                padding=1,
            )
        )
        feats_convs.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        self.feat_convs.append(nn.Sequential(*feats_convs))

        input_channels = num_ch

        for i in range(1): # set to range(2) for IMPALAx4
            resnet_block = []
            resnet_block.append(nn.ReLU())
            resnet_block.append(
                nn.Conv2d(
                    in_channels=input_channels,
                    out_channels=num_ch,
                    kernel_size=3,
                    stride=1,
                    padding=1,
                )
            )
            #resnet_block.append(nn.ReLU())
            #resnet_block.append(
            #    nn.Conv2d(
            #        in_channels=input_channels,
            #        out_channels=num_ch,
            #        kernel_size=3,
            #        stride=1,
            #        padding=1,
            #    )
            #)
            if i == 0:
                self.resnet1.append(nn.Sequential(*resnet_block))
            #else:
            #    self.resnet2.append(nn.Sequential(*resnet_block))

    self.feat_convs = nn.ModuleList(self.feat_convs)
    self.resnet1 = nn.ModuleList(self.resnet1)
    #self.resnet2 = nn.ModuleList(self.resnet2)

    self.flatten = Flatten()
    self.lin = nn.Sequential(
        nn.Linear(in_features=2048, out_features=feature_dim), nn.ReLU()
    )
    self.apply(orthogonal_init)

  def forward(self, x):
    for i, fconv in enumerate(self.feat_convs):
        x = fconv(x)
        res_input = x
        x = self.resnet1[i](x)
        x += res_input
        res_input = x
        #x = self.resnet2[i](x)
        #x += res_input
    #print("testing xshape: ", x.shape)
    x = self.flatten(x)
    #print("flatten xshape", x.shape)
    x = self.lin(x)
    return x


class Policy(nn.Module):
  def __init__(self, encoder, feature_dim, num_actions):
    super().__init__()
    self.encoder = encoder
    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)
    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)

  def act(self, x):
    with torch.no_grad():
      x = x.cuda().contiguous()
      dist, value = self.forward(x)
      action = dist.sample()
      log_prob = dist.log_prob(action)
    
    return action.cpu(), log_prob.cpu(), value.cpu()

  def forward(self, x):
    #print("input shape: ", x.shape)
    x = self.encoder(x)
    #print("afterencoder shape: ", x.shape)
    logits = self.policy(x)
    value = self.value(x).squeeze(1)
    dist = torch.distributions.Categorical(logits=logits)

    return dist, value


# Define environment
# check the utils.py file for info on arguments
env = make_env(num_envs, num_levels=num_levels)
print('Observation space:', env.observation_space)
print('Action space:', env.action_space.n)

# Define network
in_channels,_,_ = env.observation_space.shape
feature_dim = 256
num_actions = env.action_space.n
encoder = Encoder(in_channels=in_channels,feature_dim=feature_dim)
policy = Policy(encoder=encoder,feature_dim=feature_dim,num_actions=num_actions)
policy.cuda()

# Define optimizer
# these are reasonable values but probably not optimal
optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)

# Define temporary storage
# we use this to collect transitions during each iteration
storage = Storage(
    env.observation_space.shape,
    num_steps,
    num_envs
)

# Run training
obs = env.reset()
reward_storage = 0
step_storage = 0
step = 0
while step < total_steps:

  # Use policy to collect data for num_steps steps
  policy.eval()
  for _ in range(num_steps):
    # Use policy
    action, log_prob, value = policy.act(obs)
    
    # Take step in environment
    next_obs, reward, done, info = env.step(action)

    # Store data
    storage.store(obs, action, reward, done, info, log_prob, value)
    
    # Update current observation
    obs = next_obs

  # Add the last observation to collected data
  _, _, value = policy.act(obs)
  storage.store_last(obs, value)

  # Compute return and advantage
  storage.compute_return_advantage()

  # Optimize policy
  policy.train()
  for epoch in range(num_epochs):

    # Iterate over batches of transitions
    generator = storage.get_generator(batch_size)
    for batch in generator:
      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch

      # Get current policy outputs
      new_dist, new_value = policy(b_obs)
      new_log_prob = new_dist.log_prob(b_action)

      # Clipped policy objective
      ratio = torch.exp(new_log_prob - b_log_prob)
      clipped_ratio = ratio.clamp(min=1.0 - eps,
                                  max=1.0 + eps)
      policy_reward = torch.min(ratio * b_advantage,
                                clipped_ratio * b_advantage)
      pi_loss = -policy_reward.mean()

      # Clipped value function objective
      clipped_value = b_value + (new_value-b_value).clamp(min=-eps, max=eps)
      vf_loss = torch.max((new_value-b_returns)**2, (clipped_value - b_returns)**2)
      value_loss = vf_loss.mean()

      # Entropy loss
      entropy_loss = new_dist.entropy().mean()

      # Backpropagate losses
      loss = pi_loss + value_coef*value_loss + entropy_coef*entropy_loss
      loss.backward()

      # Clip gradients
      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)

      # Update policy
      optimizer.step()
      optimizer.zero_grad()

  # Update stats
  step += num_envs * num_steps
  print(f'Step: {step}\tMean reward: {storage.get_reward()}')
  reward_storage = np.append(reward_storage, storage.get_reward())
  step_storage = np.append(step_storage, step)
print('Completed training!')
torch.save(policy.state_dict, 'checkpoint.pt')

# saving training vectors
exp_version = "IMPALAx1"

if not os.path.exists('./experiments'):
    os.makedirs('./experiments')

df = pd.DataFrame({"steps": step_storage, "rewards": reward_storage})
df.to_csv(path_or_buf="./experiments/training_data_%s.csv" %exp_version, index=False)

plt.plot(step_storage, reward_storage)
plt.xlabel("step")
plt.ylabel("Mean reward")
plt.savefig("./experiments/training_curves_%s.png" %exp_version, format="png")


"""Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."""

# Make evaluation environment
eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels)
obs = eval_env.reset()

frames = []
total_reward = []

# Evaluate policy
policy.eval()
for _ in range(512):

  # Use policy
  action, log_prob, value = policy.act(obs)

  # Take step in environment
  obs, reward, done, info = eval_env.step(action)
  total_reward.append(torch.Tensor(reward))

  # Render environment and store
  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()
  frames.append(frame)

# Calculate average return
total_reward = torch.stack(total_reward).sum(0).mean(0)
print('Average return:', total_reward)

# Save frames as video
frames = torch.stack(frames)
imageio.mimsave('./experiments/vid_starpilot_%s.mp4' %exp_version, frames, fps=25)

# files.download('vid_starpilot.mp4') 
# files.download('Rewards.png')
