{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhpsqn0OMwnh"
      },
      "source": [
        "\"\"\" Instalations and libraries \"\"\"\r\n",
        "\r\n",
        "#!pip install procgen\r\n",
        "#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\r\n",
        "!wget https://github.com/raulorteg/deep-learning-project/blob/master/Models/utils.py\r\n",
        "!wget https://github.com/raulorteg/deep-learning-project/blob/master/requirements.txt\r\n",
        "!pip install -r requirements.txt\r\n",
        "\r\n",
        "import utils\r\n",
        "import argparse\r\n",
        "import json\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import imageio\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "from utils import make_env, Storage, orthogonal_init\r\n",
        "# from google.colab import files\r\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHA4y2JlMovT"
      },
      "source": [
        "print('Number of arguments: %d' % len(sys.argv))\r\n",
        "print('Argument List: %s' % str(sys.argv))\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('--env_name', type=str)\r\n",
        "parser.add_argument('--total_steps', type=float)\r\n",
        "parser.add_argument('--num_envs', type=float)\r\n",
        "parser.add_argument('--num_levels', type=float)\r\n",
        "parser.add_argument('--num_steps', type=float)\r\n",
        "parser.add_argument('--num_epochs', type=float)\r\n",
        "parser.add_argument('--start_level', type=str)\r\n",
        "parser.add_argument('--distribution_mode', type=str)\r\n",
        "parser.add_argument('--use_backgrounds', type=float)\r\n",
        "parser.add_argument('--batch_size', type=float)\r\n",
        "parser.add_argument('--eps', type=float)\r\n",
        "parser.add_argument('--grad_eps', type=float)\r\n",
        "parser.add_argument('--value_coef', type=float)\r\n",
        "parser.add_argument('--entropy_coef', type=float)\r\n",
        "\r\n",
        "hyperparameters = parser.parse_args()\r\n",
        "\r\n",
        "print(hyperparameters)\r\n",
        "\r\n",
        "\"\"\" Hyperparameters \"\"\"\r\n",
        "\r\n",
        "env_name = hyperparameters.env_name\r\n",
        "total_steps = int(hyperparameters.total_steps)\r\n",
        "num_envs = int(hyperparameters.num_envs)\r\n",
        "num_levels = int(hyperparameters.num_levels)\r\n",
        "num_steps = int(hyperparameters.num_steps)\r\n",
        "num_epochs = int(hyperparameters.num_epochs)\r\n",
        "start_level = int(hyperparameters.start_level)\r\n",
        "distribution_mode = hyperparameters.distribution_mode\r\n",
        "use_backgrounds = bool(hyperparameters.use_backgrounds)\r\n",
        "batch_size = int(hyperparameters.batch_size)\r\n",
        "eps = hyperparameters.eps\r\n",
        "grad_eps = hyperparameters.grad_eps\r\n",
        "value_coef = hyperparameters.value_coef\r\n",
        "entropy_coef = hyperparameters.entropy_coef\r\n",
        "\r\n",
        "class Flatten(nn.Module):\r\n",
        "    def forward(self, x):\r\n",
        "        return x.view(x.size(0), -1)\r\n",
        "\r\n",
        "\"\"\" IMPALA encoder \"\"\"\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self, in_channels, feature_dim):\r\n",
        "    super().__init__()\r\n",
        "    self.feat_convs = []\r\n",
        "    self.resnet1 = []\r\n",
        "    self.resnet2 = []\r\n",
        "\r\n",
        "    self.convs = []\r\n",
        "    input_channels = in_channels \r\n",
        "    for num_ch in [16, 32, 32]:\r\n",
        "        feats_convs = []\r\n",
        "        feats_convs.append(\r\n",
        "            nn.Conv2d(\r\n",
        "                in_channels=input_channels,\r\n",
        "                out_channels=num_ch,\r\n",
        "                kernel_size=3,\r\n",
        "                stride=1,\r\n",
        "                padding=1,\r\n",
        "            )\r\n",
        "        )\r\n",
        "        feats_convs.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\r\n",
        "        self.feat_convs.append(nn.Sequential(*feats_convs))\r\n",
        "\r\n",
        "        input_channels = num_ch\r\n",
        "\r\n",
        "        for i in range(2): # set to range(2) for IMPALAx4\r\n",
        "            resnet_block = []\r\n",
        "            resnet_block.append(nn.ReLU())\r\n",
        "            resnet_block.append(\r\n",
        "                nn.Conv2d(\r\n",
        "                    in_channels=input_channels,\r\n",
        "                    out_channels=num_ch,\r\n",
        "                    kernel_size=3,\r\n",
        "                    stride=1,\r\n",
        "                    padding=1,\r\n",
        "                )\r\n",
        "            )\r\n",
        "            resnet_block.append(nn.ReLU())\r\n",
        "            resnet_block.append(\r\n",
        "                nn.Conv2d(\r\n",
        "                    in_channels=input_channels,\r\n",
        "                    out_channels=num_ch,\r\n",
        "                    kernel_size=3,\r\n",
        "                    stride=1,\r\n",
        "                    padding=1,\r\n",
        "                )\r\n",
        "            )\r\n",
        "            if i == 0:\r\n",
        "                self.resnet1.append(nn.Sequential(*resnet_block))\r\n",
        "            else:\r\n",
        "                self.resnet2.append(nn.Sequential(*resnet_block))\r\n",
        "\r\n",
        "    self.feat_convs = nn.ModuleList(self.feat_convs)\r\n",
        "    self.resnet1 = nn.ModuleList(self.resnet1)\r\n",
        "    self.resnet2 = nn.ModuleList(self.resnet2)\r\n",
        "\r\n",
        "    self.flatten = Flatten()\r\n",
        "    self.lin = nn.Sequential(\r\n",
        "        nn.Linear(in_features=2048, out_features=feature_dim), nn.ReLU()\r\n",
        "    )\r\n",
        "    self.apply(orthogonal_init)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    for i, fconv in enumerate(self.feat_convs):\r\n",
        "        x = fconv(x)\r\n",
        "        res_input = x\r\n",
        "        x = self.resnet1[i](x)\r\n",
        "        x += res_input\r\n",
        "        res_input = x\r\n",
        "        x = self.resnet2[i](x)\r\n",
        "        x += res_input\r\n",
        "    #print(\"testing xshape: \", x.shape)\r\n",
        "    x = self.flatten(x)\r\n",
        "    #print(\"flatten xshape\", x.shape)\r\n",
        "    x = self.lin(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\"\"\" Declaration of policy and value functions of actor-critic method \"\"\"\r\n",
        "\r\n",
        "class Policy(nn.Module):\r\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\r\n",
        "    super().__init__()\r\n",
        "    self.encoder = encoder\r\n",
        "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\r\n",
        "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\r\n",
        "\r\n",
        "  def act(self, x):\r\n",
        "    with torch.no_grad():\r\n",
        "      x = x.cuda().contiguous()\r\n",
        "      dist, value = self.forward(x)\r\n",
        "      action = dist.sample()\r\n",
        "      log_prob = dist.log_prob(action)\r\n",
        "    \r\n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    #print(\"input shape: \", x.shape)\r\n",
        "    x = self.encoder(x)\r\n",
        "    #print(\"afterencoder shape: \", x.shape)\r\n",
        "    logits = self.policy(x)\r\n",
        "    value = self.value(x).squeeze(1)\r\n",
        "    dist = torch.distributions.Categorical(logits=logits)\r\n",
        "\r\n",
        "    return dist, value\r\n",
        "\r\n",
        "\r\n",
        "\"\"\" Define environment \"\"\"\r\n",
        "\r\n",
        "# check the utils.py file for info on arguments\r\n",
        "env = make_env(\r\n",
        "  num_envs,\r\n",
        "  env_name=env_name,\r\n",
        "  num_levels=num_levels,\r\n",
        "  use_backgrounds=use_backgrounds,\r\n",
        "  distribution_mode=distribution_mode)\r\n",
        "test_env = make_env(\r\n",
        "  num_envs,\r\n",
        "  env_name=env_name,\r\n",
        "  start_level=num_levels,\r\n",
        "  num_levels=num_levels,\r\n",
        "  use_backgrounds=use_backgrounds,\r\n",
        "  distribution_mode=distribution_mode,\r\n",
        "  seed=80)\r\n",
        "print('Observation space:', env.observation_space)\r\n",
        "print('Action space:', env.action_space.n)\r\n",
        "\r\n",
        "\"\"\" Define network \"\"\"\r\n",
        "\r\n",
        "in_channels,_,_ = env.observation_space.shape\r\n",
        "feature_dim = 256\r\n",
        "num_actions = env.action_space.n\r\n",
        "encoder = Encoder(in_channels=in_channels,feature_dim=feature_dim)\r\n",
        "policy = Policy(encoder=encoder,feature_dim=feature_dim,num_actions=num_actions)\r\n",
        "policy.cuda()\r\n",
        "\r\n",
        "\"\"\" Define network and optimizer \"\"\"\r\n",
        "\r\n",
        "# these are reasonable values but probably not optimal\r\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\r\n",
        "\r\n",
        "# Define temporary storage\r\n",
        "# we use this to collect transitions during each iteration\r\n",
        "storage = Storage(\r\n",
        "    env.observation_space.shape,\r\n",
        "    num_steps,\r\n",
        "    num_envs\r\n",
        ")\r\n",
        "\r\n",
        "# Define network\r\n",
        "in_channels,_,_ = env.observation_space.shape\r\n",
        "feature_dim = 256\r\n",
        "num_actions = env.action_space.n\r\n",
        "encoder = Encoder(in_channels=in_channels,feature_dim=feature_dim)\r\n",
        "policy = Policy(encoder=encoder,feature_dim=feature_dim,num_actions=num_actions)\r\n",
        "policy.cuda()\r\n",
        "\r\n",
        "# Define optimizer\r\n",
        "# these are reasonable values but probably not optimal\r\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\r\n",
        "\r\n",
        "# Define temporary storage\r\n",
        "# we use this to collect transitions during each iteration\r\n",
        "storage = Storage(\r\n",
        "    env.observation_space.shape,\r\n",
        "    num_steps,\r\n",
        "    num_envs\r\n",
        ")\r\n",
        "\r\n",
        "\"\"\" Run training \"\"\"\r\n",
        "obs = env.reset()\r\n",
        "test_obs = test_env.reset()\r\n",
        "reward_storage = 0\r\n",
        "std_storage = 0\r\n",
        "test_reward_storage = 0\r\n",
        "test_std_storage = 0\r\n",
        "step_storage = 0\r\n",
        "\r\n",
        "step = 0\r\n",
        "while step < total_steps:\r\n",
        "\r\n",
        "  # Use policy to collect data for num_steps steps\r\n",
        "  policy.eval()\r\n",
        "  for _ in range(num_steps):\r\n",
        "    # Use policy\r\n",
        "    action, log_prob, value = policy.act(obs)\r\n",
        "    test_action, test_log_prob, test_value = policy.act(test_obs)\r\n",
        "    \r\n",
        "    # Take step in environment\r\n",
        "    next_obs, reward, done, info = env.step(action)\r\n",
        "    test_next_obs, test_reward, test_done, test_info = test_env.step(test_action)\r\n",
        "\r\n",
        "    # Store data\r\n",
        "    storage.store(obs, action, reward, done, info, log_prob, value)\r\n",
        "    storage.test_store(test_info)\r\n",
        "    \r\n",
        "    # Update current observation\r\n",
        "    obs = next_obs\r\n",
        "    test_obs = test_next_obs\r\n",
        "\r\n",
        "  # Add the last observation to collected data\r\n",
        "  _, _, value = policy.act(obs)\r\n",
        "  storage.store_last(obs, value)\r\n",
        "\r\n",
        "  # Compute return and advantage\r\n",
        "  storage.compute_return_advantage()\r\n",
        "\r\n",
        "  # Optimize policy\r\n",
        "  policy.train()\r\n",
        "  for epoch in range(num_epochs):\r\n",
        "\r\n",
        "    # Iterate over batches of transitions\r\n",
        "    generator = storage.get_generator(batch_size)\r\n",
        "    for batch in generator:\r\n",
        "      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\r\n",
        "\r\n",
        "      # Get current policy outputs\r\n",
        "      new_dist, new_value = policy(b_obs)\r\n",
        "      new_log_prob = new_dist.log_prob(b_action)\r\n",
        "\r\n",
        "      # Clipped policy objective      \r\n",
        "      ratio = torch.exp(new_log_prob - b_log_prob)\r\n",
        "      surr1 = ratio * b_advantage\r\n",
        "      surr2 = torch.clamp(ratio, 1.0 - eps, 1.0 + eps) * b_advantage\r\n",
        "      pi_loss = -torch.min(surr1, surr2).mean()\r\n",
        "\r\n",
        "      # Clipped value function objective\r\n",
        "      clipped_value = b_value + (new_value-b_value).clamp(min=-eps, max=eps)\r\n",
        "      vf_loss = torch.max((new_value-b_returns).pow(2), (clipped_value - b_returns).pow(2))\r\n",
        "      value_loss = vf_loss.mean()\r\n",
        "\r\n",
        "      # Entropy loss\r\n",
        "      entropy_loss = new_dist.entropy().mean()\r\n",
        "\r\n",
        "      # Backpropagate losses\r\n",
        "      loss = pi_loss + value_coef*value_loss - entropy_coef*entropy_loss\r\n",
        "      loss.backward()\r\n",
        "\r\n",
        "      # Clip gradients\r\n",
        "      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\r\n",
        "\r\n",
        "      # Update policy\r\n",
        "      optimizer.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "  # Update stats\r\n",
        "  step += num_envs * num_steps\r\n",
        "  \r\n",
        "  ## Get train mean reward\r\n",
        "  reward_storage = np.append(reward_storage, storage.get_reward())\r\n",
        "  print(f'Step: {step}\\tMean train reward: {reward_storage[-1]}')\r\n",
        "  std_storage = np.append(std_storage, np.std(reward_storage))\r\n",
        "  \r\n",
        "  ## Get test mean reward\r\n",
        "  test_reward_storage = np.append(test_reward_storage, storage.get_test_reward())\r\n",
        "  print(f'Step: {step}\\tMean test reward: {test_reward_storage[-1]}\\n')\r\n",
        "  test_std_storage = np.append(test_std_storage, np.std(test_reward_storage))\r\n",
        "  \r\n",
        "  step_storage = np.append(step_storage, step)\r\n",
        "print('Completed training!')\r\n",
        "torch.save(policy.state_dict, 'checkpoint.pt')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsL3BoA7NEWQ"
      },
      "source": [
        "\"\"\" Save, plot, training and test rewards \"\"\"\r\n",
        "\r\n",
        "exp_version = \"starpilot_testplot\"\r\n",
        "\r\n",
        "if not os.path.exists('./experiments'):\r\n",
        "    os.makedirs('./experiments')\r\n",
        "\r\n",
        "# Training data\r\n",
        "df = pd.DataFrame({\"steps\": step_storage, \"rewards\": reward_storage})\r\n",
        "df.to_csv(path_or_buf=\"./experiments/training_data_%s.csv\" %exp_version, index=False)\r\n",
        "plt.plot(step_storage, reward_storage, color=\"#5E35B1\", label = \"Train\")\r\n",
        "plt.fill_between(step_storage, reward_storage+std_storage, reward_storage-std_storage,\r\n",
        "                 color=\"#5E35B1\", edgecolor=\"#FFFFFF\", alpha=0.2)\r\n",
        "# Test data\r\n",
        "df_test = pd.DataFrame({\"steps\": step_storage, \"rewards\": test_reward_storage})\r\n",
        "df_test.to_csv(path_or_buf=\"./experiments/test_data_%s.csv\" %exp_version, index=False)\r\n",
        "plt.plot(step_storage, test_reward_storage, color=\"#FF6F00\", label = \"Test\")\r\n",
        "plt.fill_between(step_storage, test_reward_storage+test_std_storage, test_reward_storage-test_std_storage,\r\n",
        "                  color = \"#FF6F00\", edgecolor=\"#FFFFFF\", alpha=0.2)\r\n",
        "\r\n",
        "plt.legend(loc=4)\r\n",
        "plt.xlabel(\"Step\")\r\n",
        "plt.ylabel(\"Mean reward\")\r\n",
        "plt.savefig(\"./experiments/Reward_curves_%s.png\" %exp_version, format=\"png\")\r\n",
        "\r\n",
        "\r\n",
        "\"\"\" Visualize performance on a test level \"\"\"\r\n",
        "\r\n",
        "# Make evaluation environment\r\n",
        "obs = env.reset()\r\n",
        "\r\n",
        "frames = []\r\n",
        "total_reward = []\r\n",
        "\r\n",
        "# Evaluate policy\r\n",
        "policy.eval()\r\n",
        "for _ in range(1024):\r\n",
        "\r\n",
        "  # Use policy\r\n",
        "  action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "  # Take step in environment\r\n",
        "  obs, reward, done, info = env.step(action)\r\n",
        "  total_reward.append(torch.Tensor(reward))\r\n",
        "\r\n",
        "  # Render environment and store\r\n",
        "  frame = (torch.Tensor(env.render(mode='rgb_array'))*255.).byte()\r\n",
        "  frames.append(frame)\r\n",
        "\r\n",
        "# Calculate average return\r\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\r\n",
        "print('Average test return:', total_reward)\r\n",
        "\r\n",
        "# Save frames as video\r\n",
        "frames = torch.stack(frames)\r\n",
        "imageio.mimsave('./experiments/vid_train_%s.mp4' %exp_version, frames, fps=45)\r\n",
        "\r\n",
        "\"\"\" Visualize performance on a test level \"\"\"\r\n",
        "\r\n",
        "# Make evaluation environment\r\n",
        "eval_env = make_env(\r\n",
        "  num_envs,\r\n",
        "  env_name=env_name,\r\n",
        "  start_level=num_levels,\r\n",
        "  num_levels=num_levels,\r\n",
        "  use_backgrounds=use_backgrounds,\r\n",
        "  distribution_mode=distribution_mode)\r\n",
        "  \r\n",
        "obs = eval_env.reset()\r\n",
        "\r\n",
        "frames = []\r\n",
        "total_reward = []\r\n",
        "\r\n",
        "# Evaluate policy\r\n",
        "policy.eval()\r\n",
        "for _ in range(1024):\r\n",
        "\r\n",
        "  # Use policy\r\n",
        "  action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "  # Take step in environment\r\n",
        "  obs, reward, done, info = eval_env.step(action)\r\n",
        "  total_reward.append(torch.Tensor(reward))\r\n",
        "\r\n",
        "  # Render environment and store\r\n",
        "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\r\n",
        "  frames.append(frame)\r\n",
        "\r\n",
        "# Calculate average return\r\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\r\n",
        "print('Average test return:', total_reward)\r\n",
        "\r\n",
        "# Save frames as video\r\n",
        "frames = torch.stack(frames)\r\n",
        "imageio.mimsave('./experiments/vid_test_%s.mp4' %exp_version, frames, fps=45)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}